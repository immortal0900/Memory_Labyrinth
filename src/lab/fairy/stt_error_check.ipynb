{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32dfea17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from pathlib import Path\n",
    "\n",
    "def find_src_folder():\n",
    "    current = Path(os.getcwd()).resolve()\n",
    "    for p in [current] + list(current.parents):\n",
    "        src = p / \"src\"\n",
    "        if src.exists():\n",
    "            return src\n",
    "    raise RuntimeError(\"src 폴더를 찾을 수 없습니다.\")\n",
    "\n",
    "src_path = find_src_folder()\n",
    "sys.path.append(str(src_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f6a12661",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "너는 한국어 STT 결과가 \"인지 가능\"인지 \"인지 불가\"인지\n",
    "이진 분류용 학습데이터를 생성하는 역할이다.\n",
    "\n",
    "데이터는 (stt_text, label) 쌍으로만 구성된다.\n",
    "\n",
    "판정 기준은 오직 하나다.\n",
    "- stt_text만 단독으로 보았을 때,\n",
    "  사람이 어떤 질문을 하려는지 감이 오면 label=0\n",
    "- 감이 오지 않으면 label=1\n",
    "\n",
    "문법, 맞춤법, 조사, 띄어쓰기, 단어 치환은 전혀 중요하지 않다.\n",
    "틀려도 된다.\n",
    "짧아도 된다.\n",
    "\n",
    "중요한 것은 오직 이것이다:\n",
    "→ \"사람이 의미를 떠올릴 수 있느냐\"\n",
    "\n",
    "아래와 같은 형태는 반드시 label=0이다.\n",
    "- \"이벤터 어또케\"\n",
    "- \"몬스터 모야\"\n",
    "- \"인벤 뭐있어\"\n",
    "- \"보스방 어케가\"\n",
    "\n",
    "출력은 JSONL 형식만 허용한다.\n",
    "설명, 주석, 추가 텍스트는 출력하지 않는다.\n",
    "\"\"\"\n",
    "\n",
    "human_prompt_0 = \"\"\"\n",
    "이번에 생성하는 모든 샘플의 label은 반드시 0이다.\n",
    "\n",
    "아래 조건을 만족하는 한국어 stt_text를 생성하라.\n",
    "\n",
    "조건:\n",
    "- STT 인식 결과처럼 보이는 문장\n",
    "- 발음이 찌그러지거나 단어가 틀려도 상관없다\n",
    "- 조사 누락, 축약, 띄어쓰기 붕괴 모두 허용\n",
    "- 하지만 사람이 보면\n",
    "  \"아, 이게 무슨 질문인지 알겠다\"는 느낌이 들어야 한다\n",
    "- 시스템 프롬프트를 참고해서 세계관에 맞는 stt_text를 위주로 만들어달라\n",
    "\n",
    "의도가 느껴지면 된다.\n",
    "완벽할 필요는 없다.\n",
    "\n",
    "출력은 JSONL 한 줄만 한다.\n",
    "\n",
    "{\n",
    "  \"stt_text\": \"...\",\n",
    "  \"label\": 0\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "human_prompt_1 = \"\"\"\n",
    "이번에 생성하는 모든 샘플의 label은 반드시 1이다.\n",
    "\n",
    "중요:\n",
    "label 판단은 오직 stt_text만 보고 한다.\n",
    "비교 대상은 없다.\n",
    "정답 문장을 상상하지 마라.\n",
    "\n",
    "아래 조건을 모두 만족하는 stt_text만 생성하라.\n",
    "\n",
    "조건:\n",
    "- 사람이 봤을 때 어떤 질문인지 감이 오지 않아야 한다\n",
    "- 핵심 명사나 동작이 드러나면 실패다\n",
    "- 문장이 질문처럼 느껴지면 실패다\n",
    "- \"이게 무슨 말이지?\"라는 반응이 나와야 한다\n",
    "- 가끔 영어랑 한글이 섞여있다.\n",
    "- 한자도 섞일때가 있다.\n",
    "\n",
    "다음과 같은 형태는 절대 만들지 마라 (이건 label=0이다):\n",
    "- 인벤 아이템 무슨키지\n",
    "- 이벤트 어떻게 해\n",
    "- 몬스터 뭐야\n",
    "- 보스방 가는길\n",
    "\n",
    "반드시 더 심하게 깨뜨려라.\n",
    "\n",
    "출력은 JSONL 한 줄만 한다.\n",
    "\n",
    "{\n",
    "  \"stt_text\": \"...\",\n",
    "  \"label\": 1\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.messages import SystemMessage, HumanMessage\n",
    "from enums.LLM import LLM\n",
    "\n",
    "label0_result = ChatOpenAI(model=LLM.GPT4_1_MINI).invoke(\n",
    "    [SystemMessage(content=system_prompt), HumanMessage(content=human_prompt_0)]\n",
    ")\n",
    "label0_result.content\n",
    "\n",
    "label1_result = ChatOpenAI(model=LLM.GPT4_1_MINI).invoke(\n",
    "    [SystemMessage(content=system_prompt), HumanMessage(content=human_prompt_0)]\n",
    ")\n",
    "label1_result.content\n",
    "\n",
    "\n",
    "label0_samples = []\n",
    "label1_samples = []\n",
    "\n",
    "\n",
    "TARGET = 1000\n",
    "\n",
    "while len(label0_samples) < TARGET:\n",
    "    sample = ChatOpenAI(model=LLM.GPT4_1_MINI).invoke(\n",
    "        [SystemMessage(content=system_prompt), HumanMessage(content=human_prompt_0)]\n",
    "    )\n",
    "    if sample:\n",
    "        label0_samples.append(sample.content)\n",
    "\n",
    "\n",
    "while len(label1_samples) < TARGET:\n",
    "   sample = ChatOpenAI(model=LLM.GPT4_1_MINI).invoke([SystemMessage(content=system_prompt), HumanMessage(content=human_prompt_1)])   \n",
    "   if sample:\n",
    "        label1_samples.append(sample.content)\n",
    "\n",
    "import random, json\n",
    "all_samples = label0_samples + label1_samples\n",
    "random.shuffle(all_samples)\n",
    "\n",
    "with open(\"stt_binary_dataset.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_samples, f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f16c63ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows: 1994\n",
      "[{'text': '핑구 바람슛 甲乙丙合え', 'label': 1}, {'text': '보스 던전 어떻게 찾아', 'label': 0}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def load_rows(path=\"stt_binary_dataset.json\"):\n",
    "    raw = json.loads(Path(path).read_text(encoding=\"utf-8\"))\n",
    "    rows = []\n",
    "    for r in raw:\n",
    "        if isinstance(r, str):\n",
    "            try:\n",
    "                r = json.loads(r)\n",
    "            except:\n",
    "                continue\n",
    "        if not isinstance(r, dict): \n",
    "            continue\n",
    "        if \"stt_text\" not in r or \"label\" not in r:\n",
    "            continue\n",
    "        t = str(r[\"stt_text\"]).strip()\n",
    "        if not t:\n",
    "            continue\n",
    "        y = int(r[\"label\"])\n",
    "        if y not in (0, 1):\n",
    "            continue\n",
    "        rows.append({\"text\": t, \"label\": y})\n",
    "    return rows\n",
    "\n",
    "rows = load_rows()\n",
    "print(\"rows:\", len(rows))\n",
    "print(rows[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "636a490d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 1794/1794 [00:00<00:00, 40905.80 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 58294.70 examples/s]\n",
      "/var/folders/fx/bdj1bhw11037xfwjzgrx6vc80000gn/T/ipykernel_16031/2362458146.py:49: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2, 'bos_token_id': 0, 'pad_token_id': 1}.\n",
      "/Users/seobi/PythonProjects/ProjectML/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='87' max='87' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [87/87 00:37, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.301916</td>\n",
       "      <td>0.990000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.415000</td>\n",
       "      <td>0.128509</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.415000</td>\n",
       "      <td>0.101787</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seobi/PythonProjects/ProjectML/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/seobi/PythonProjects/ProjectML/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved: stt_miniLM_cls_final\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "\n",
    "# ✅ 추천 인코더 (한국어/영어/다국어에 강함)\n",
    "MODEL_NAME = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "\n",
    "train_rows, valid_rows = train_test_split(rows, test_size=0.1, random_state=42, stratify=[r[\"label\"] for r in rows])\n",
    "ds_train = Dataset.from_list(train_rows)\n",
    "ds_valid = Dataset.from_list(valid_rows)\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "\n",
    "def preprocess(batch):\n",
    "    return tok(batch[\"text\"], truncation=True, max_length=64)  # STT는 짧아서 64면 충분\n",
    "\n",
    "ds_train = ds_train.map(preprocess, batched=True)\n",
    "ds_valid = ds_valid.map(preprocess, batched=True)\n",
    "\n",
    "collator = DataCollatorWithPadding(tok)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    acc = (preds == labels).mean()\n",
    "    return {\"accuracy\": float(acc)}\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"stt_miniLM_cls\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=128,\n",
    "    num_train_epochs=3,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=50,\n",
    "    fp16=True,  # GPU 있으면\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=ds_train,\n",
    "    eval_dataset=ds_valid,\n",
    "    tokenizer=tok,\n",
    "    data_collator=collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"stt_miniLM_cls_final\")\n",
    "tok.save_pretrained(\"stt_miniLM_cls_final\")\n",
    "print(\"saved: stt_miniLM_cls_final\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "58ca8dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "MODEL_NAME = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_NAME)  # ✅ 여기만 원본으로\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"stt_miniLM_cls_final\").eval()\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict(text: str):\n",
    "    x = tok(text, return_tensors=\"pt\", truncation=True, max_length=64)\n",
    "    logits = model(**x).logits\n",
    "    probs = torch.softmax(logits, dim=-1)[0].cpu().numpy()\n",
    "    return probs.argmax()\n",
    "\n",
    "print(predict(\"이벤터 어또케\"))\n",
    "# print(predict(\"꽉!끼!떡!\"))\n",
    "# print(predict(\"꽉!끼!떡!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "36d926d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes_: [0 1]\n",
      "{'stt_text': '이벤터 어또케', 'pred_label': 0, 'p_label1': 0.0011737981821645304}\n",
      "{'stt_text': '핑구 바람슛 甲乙丙合え', 'pred_label': 1, 'p_label1': 0.9963839720056229}\n",
      "{'stt_text': '인벤 아이템 무슨키지', 'pred_label': 0, 'p_label1': 0.0016478660884164428}\n",
      "{'stt_text': '적긔모', 'pred_label': 1, 'p_label1': 0.9839471982250018}\n",
      "{'stt_text': '저 몬스터는 뭐야?', 'pred_label': 0, 'p_label1': 0.13074811606145645}\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "model = joblib.load(\"stt_char_tfidf_svm_cal.joblib\")\n",
    "\n",
    "print(\"classes_:\", model.classes_)  # 예: [0 1]\n",
    "\n",
    "def p_label1(text: str) -> float:\n",
    "    proba = model.predict_proba([text])[0]\n",
    "    idx = list(model.classes_).index(1)  # 라벨 1 위치를 안전하게 찾기\n",
    "    return float(proba[idx])\n",
    "\n",
    "def predict_one(text: str, threshold=0.5):\n",
    "    p1 = p_label1(text)\n",
    "    pred = 1 if p1 >= threshold else 0\n",
    "    return {\"stt_text\": text, \"pred_label\": pred, \"p_label1\": p1}\n",
    "\n",
    "print(predict_one(\"이벤터 어또케\"))           # 기대: pred_label=0\n",
    "print(predict_one(\"핑구 바람슛 甲乙丙合え\"))   # 기대: pred_label=1\n",
    "print(predict_one(\"인벤 아이템 무슨키지\"))     # 기대: pred_label=0\n",
    "print(predict_one(\"적긔모\"))                   # 기대: pred_label=1\n",
    "print(predict_one(\"저 몬스터는 뭐야?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8cd293",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ProjectML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
